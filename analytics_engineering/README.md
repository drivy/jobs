# Getaround EU Analytics Engineering Challenge (previously Drivy)

Looking for a job? Check out our [open positions](https://www.welcometothejungle.com/en/companies/getaround/jobs).
You can also take a look at our [engineering blog](https://drivy.engineering/) to learn more about the way we work.

## General guidelines

- Clone this repo (do **not** fork it)
- Solve the exercises in  ascending order
- Only do one commit per exercise, include the `.git` when submiting your work

Please do the simplest thing that could work for the exercise you're currently solving.

For higher levels we are interested in seeing code that is:

- Clean
- Extensible
- Reliable

## Challenge specifications

The challenge needs to be resolved in PostgreSQL.
Exercises depend on datasets available in `./data/*`
**You can't modify them.**

Your solution to exercise {N} needs to live in the `worksheets_{N}` directory as .sql files. If you need to write multiple SQL queries for an answer, separate them with `;` characters, a linebreak and 2 spaces.
**The SQL queries you provide need to be working.**

For certain parts of the challenge, you will be asked to provide written answers, in these cases, please submit them in `worksheets_{N}` as markdown files.


## Exercise 0: Setup and brief

For this challenge, you are provided with payloads that have been generated whenever a user triggers an action relevant to our analytics on the platform. They are available at `./data/*`

Each event you have parsed out of these raw payloads respects a default structure:
- `timestamp`: generated at the exact time of the action.
- `anonymous_id`: associated to a user device
- `action_id`: associated to the action

With an optional additional key:
- `url`: url the user has triggered the action from

**DISCLAIMER**: these data are randomly faked end to end, do not try to interpret them beyond exercises instructions.

### 0.1

Create a local PostgreSQL instance

### 0.2

Create a table named `raw_payloads` in the SQL instance you have just created and insert the payloads provided in `./data/tracking_payloads_20210603.csv`.

### 0.3

Parse the json objects you have inserted in `raw_payloads`: each key should become a column, each value should become a row value.
With your parsing query, create another table named `events` and insert your parsed version of `raw_payloads` inside of it.


## Exercise 1: Sessions

We want to aggregate events together to analyse user behavior over a certain period of time.

### 1.1 

Create a table that aggregates events per session of events. Sessions are segmented period of activity we observe for a user. In this exercise, we consider that if a user stops their activity for more than 30 minutes, the session as ended. If a user starts their activity again, then a new session begins until the user stops for more than 30 minutes again.

The SQL query you design to create this table should ultimately respect the following format:
- `anonymous_id`
- `session_id`
- `session_start`
- `session_end`

Explain your design in your markdown file.

### 1.2 

We now want to know which marketing action has attracted the user to begin their session. Thanksfully, `url` provides us with `utm_source`, `utm_medium` and `utm_campaign` details we can use to find out. For more details about UTM, check out this [blog post](https://buffer.com/library/utm-guide/)

Enrich your `session` table with a `source` column that gives your detail about the marketing action associated to each `session_id`.


## Exercise 2: Incremental pipeline

Another set of raw payloads is provided to you in `./data/tracking_payloads_20210604.csv`.

### 2.1

Write the serie of queries that will succesively update `raw_payloads`, `events` and `sessions` in your SQL file.

Explain your approach in the markdown file.

### 2.2

Do you observe issues in your computations because of this additional file? 

If so, create a second SQL file to show us how you would adapt your transformation queries to produce reliable tables for analytics.

Lkist the issues you have noticed and explain how your design fixes them in your markdown file.


## Exercise 3: Challenge performance

In reality, millions of payloads are generated by platforms like ours everyday.

Without any additional dataset, find a way to challenge your query with a larger volume of data (> 1000 rows). Write your workaround methods in a SQL file.

Explain your approach in the markdown file.


## Exercise 4: Tests

Write queries that you could run to test your raw and transformed tables.

The test should challenge the quality and consistency of the data you are provided with and the data you have transformed. Example: you could run a test to check if payloads have been duplicated.

In your markdown file, tell us how and when the test queries you have written would be executed to optimally test your data. Tell us why you think these tests are essential in the context of the story of this exercise.


## Conclusion

When you are done with this challenge, create a private repository and submit it to your interviewer.