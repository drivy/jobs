# Getaround EU Analytics Engineering Challenge (previously Drivy)

Looking for a job? Check out our [open positions](https://www.welcometothejungle.com/en/companies/getaround/jobs).
You can also take a look at our [engineering blog](https://drivy.engineering/) to learn more about the way we work.

## General guidelines

- Clone this repo (do **not** fork it)
- Solve the exercises in  ascending order
- Only do one commit per exercise

Please do the simplest thing that could work for the exercise you're currently solving.

For higher levels applicants we are interested in seeing code that is:

- Clean
- Extensible
- Reliable

## Challenge specifications

The challenge needs to be resolved in PostgreSQL.
Exercises depend on datasets available in `./data/*`
**You can't modify them.**

Your solution to exercise {N} needs to live in the `worksheets_{N}` directory as .sql files. If you need to write multiple SQL queries for an answer, separate them with `;` characters, a linebreak and 2 spaces.
**The SQL queries you provide need to be working.**

For certain parts of the challenge, you will be asked to provide written answers, in these cases, please submit them in `worksheets_{N}` as markdown files. Make sure you use these files for each exercise. Everything should be written in English.


## Exercise 1: Setup and brief

For this challenge, you are provided with payloads that have been generated whenever a user triggers an action relevant to our analytics on the platform. They are available at `./data/*`

Each event you have parsed out of these raw payloads respects a default structure:
- `timestamp`: generated at the exact time of the action
- `anonymous_id`: associated to a user device
- `action_id`: associated to the action

With an optional additional key:
- `url`: url the user has triggered the action from

**DISCLAIMER**: these data are randomly faked end to end

Note: we advise to use PostgreSQL but any SQL technology should work as long as you document the commands you have run. Just make sure you are able to execute the query you are submitting in the case to test them.

### 1.1

Create a local PostgreSQL instance

### 1.2

Create a table named `raw_payloads` in the SQL instance you have just created and insert the payloads provided in `./data/tracking_payloads_20210603.csv`.

### 1.3

Parse the json objects you have inserted in `raw_payloads`: each key should become a column, each value should become a row value.
With your parsing query, create another table named `events` and insert your parsed version of `raw_payloads` inside of it.


## Exercise 2: Sessions

We want to aggregate events together to analyse user behavior over a certain period of time.

### 2.1 

Create a table that aggregates events per session of events. Sessions are segmented period of activity we observe for a user. In this exercise, we consider that if a user stops their activity for more than 30 minutes, the session has ended. If a user starts their activity again, then a new session begins until the user stops for more than 30 minutes again.

The SQL query you design to create this table should ultimately respect the following format:
- `anonymous_id`
- `session_id`
- `session_start`
- `session_end`

Explain your design in your markdown file.

### 2.2 

We now want to know which marketing campaign has attracted the user to begin their session. Thanksfully, `url` provides us with `utm_source`, `utm_medium` and `utm_campaign` details we can use to find out. For more details about UTMs, check out this [blog post](https://buffer.com/library/utm-guide/)

Enrich your `session` table with a `utm_source` column that provides you with information about the marketing campaigns that can be associated to each `session_id`.

Explain your choice and approach to associate sessions with utm_sources.


## Exercise 3: Incremental pipeline

Another set of raw payloads is provided to you in `./data/tracking_payloads_20210604.csv`.

### 3.1

Write the serie of queries that would succesively update `raw_payloads`, `events` and `sessions` in your SQL file. If that is possible, try to avoid recomputing everything.

Explain your approach in the markdown file.

### 3.2

Did you observe issues because of this additional file? 

If so, create a second SQL file to show us how you would adapt your transformation queries to produce a reliable session table for analytics purposes.

List the issues you have noticed and explain how your new design fixes them in your markdown file.


## Exercise 4: Challenge performance

As you can guess, in reality, millions of payloads are generated by platforms like Getaround everyday.

Find a way to challenge your query with a larger volume of data (> 1000 rows). Write your workaround methods in a SQL or Python file.

Explain your approach in the markdown file.


## Exercise 5: Tests

Data quality matters for analytics.

### 5.1 

Write queries that you could run to test your raw AND transformed tables.
The test should challenge the quality and consistency of the data you are provided with and the data you have transformed.

### 5.2

Finally, in your markdown file, tell us how and when the test queries you have written would be executed to optimally test your data. Tell us why you think these tests are essential in the context of the story of this exercise.


## Conclusion

When you are done with this challenge, create a private repository and submit it to your interviewer who should have provided you with their GitHub username. Let them know by email!
